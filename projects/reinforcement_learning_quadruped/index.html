<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Reinforcement Learning for Quadruped | Shail Dalal Portfolio</title> <meta name="author" content="Shail Dalal"> <meta name="description" content="Weaving poles using a Unitree Go1"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sdalal1.github.io/projects/reinforcement_learning_quadruped/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Shail Dalal Portfolio</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reinforcement Learning for Quadruped</h1> <p class="post-description">Weaving poles using a Unitree Go1</p> </header> <article> <h2 id="project-quadruped-reinforcement-learning-for-dynamic-weaving-on-unitree-go-1">Project: Quadruped Reinforcement Learning for Dynamic Weaving on Unitree Go 1</h2> <h4 id="description"><strong>Description:</strong></h4> <p>The objective of this project is to replicate the intricate weaving motion performed by animals around poles using reinforcement learning with a quadruped. Inspired by one of the most common tricks exhibited by animals, I aim to emulate this movement pattern with precision and agility. To achieve this, I’ve chosen to utilize reinforcement learning within the Nvidia IsaacGym environment, paired with the versatile Unitree Go1 quadruped. Through this setup, I seek to unravel the complexities of animal-like locomotion and push the boundaries of what’s achievable in robotic motion control.</p> <h4 id="training"><strong>Training:</strong></h4> <p>The training is a two step process and they are as follows:</p> <ul> <li> <h5 id="base-policy"><strong>Base-Policy</strong></h5> </li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/202403060006.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> </div> <div class="caption"> Base policy setup with known scandots location and heading </div> <p>The base policy is trained within the simulation environment using all available data, particularly focusing on goal positions and headings. Scan-dot positions serve as primary objectives, with rewards granted upon successful attainment by the quadruped. However, collisions and exceeding joint/torque limits result in penalties within the simulation, driving the training process to refine joint states for optimal task performance.</p> <ul> <li> <h5 id="distillation-policy"><strong>Distillation Policy</strong></h5> </li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/Unitree_distillation.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> </div> <div class="caption"> Distillation video of Unitree performing weaving </div> <p>The second step involves training the Distillation policy, which is constructed as a layer atop the base policy. In this phase, joint states trained by the base policy are stored in history and utilized for further refinement. Unlike the base policy, the Distillation policy relies solely on camera input, without access to privileged knowledge such as scan-dots and heading. Through this setup, the policy aims to determine the optimal heading relative to obstacles using joint states and camera data.</p> <p>The quadruped receives consistent rewards and penalties to ensure the attainment of a robust output. Once trained, this policy is saved as a pytorch-jit model for seamless deployment onto the real robot.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mean_reward_base-480.webp 480w,/assets/img/mean_reward_base-800.webp 800w,/assets/img/mean_reward_base-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/mean_reward_base.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Base Policy Rewards" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/training_reward_dist-480.webp 480w,/assets/img/training_reward_dist-800.webp 800w,/assets/img/training_reward_dist-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/training_reward_dist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Distillation Policy Rewards" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left is the Base policy reward curve and the right is Distillation policy reward curve </div> <p>The training flowchart is as follows:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/flowchart-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/flowchart-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/flowchart-1400.webp"></source> <img src="/assets/img/flowchart.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="policy flowchart" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>During training, meticulous attention is paid to the loss graphs to monitor the policy’s progress. When the loss converges, indicating satisfactory training, several steps follow. Firstly, the trained policy is tested to ensure the proper functionality of the joints. After successful validation, training may resume for distillation or the policy may be saved for deployment on the real robot. This careful process ensures that the policy undergoes thorough validation before advancing, ensuring reliability and performance in real-world applications.</p> <div class="row"> <div class="col"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/estimator_loss_base-480.webp 480w,/assets/img/estimator_loss_base-800.webp 800w,/assets/img/estimator_loss_base-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/estimator_loss_base.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Base Policy Rewards" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/latent_loss_png_base-480.webp 480w,/assets/img/latent_loss_png_base-800.webp 800w,/assets/img/latent_loss_png_base-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/latent_loss_png_base.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Distillation Policy Rewards" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Base policy loss graphs </div> </div> <div class="col"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/depth_actor_loss-480.webp 480w,/assets/img/depth_actor_loss-800.webp 800w,/assets/img/depth_actor_loss-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/depth_actor_loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Base Policy Rewards" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/depth_loss_yaw-480.webp 480w,/assets/img/depth_loss_yaw-800.webp 800w,/assets/img/depth_loss_yaw-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/depth_loss_yaw.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Distillation Policy Rewards" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Distillation policy loss graphs </div> </div> </div> <h4 id="terrain-manipulation"><strong>Terrain Manipulation</strong></h4> <p>Creating the gym terrain to simulate the poles for weaving was a crucial step in this project. Multiple environments were meticulously set up to facilitate training, with each environment playing a unique role in the learning process. Training was conducted on a remote machine using Docker, ensuring scalability and efficient resource utilization.</p> <p>The terrain design is of paramount importance as it directly influences the interaction between the quadruped and its surroundings, thereby shaping the actor training process. By accurately representing the obstacles and challenges that the quadruped encounters, the terrain enables the actor to learn and adapt its behavior accordingly. This emphasis on realistic terrain simulation enhances the training process, leading to more robust and effective policies.</p> <div class="row"> <div class="col"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/raw_terrain-480.webp 480w,/assets/img/raw_terrain-800.webp 800w,/assets/img/raw_terrain-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/raw_terrain.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Raw terrain" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Raw Terrain </div> </div> <div class="col"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/multiple_actors-480.webp 480w,/assets/img/multiple_actors-800.webp 800w,/assets/img/multiple_actors-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/multiple_actors.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multiple Actors Training" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Multiple actors training </div> </div> </div> <h4 id="sim-to-real"><strong>SIM TO REAL</strong></h4> <p>The ongoing effort to transition from simulation to real-life deployment is a critical aspect of this project. Leveraging ROS2 for deployment underscores the commitment to robust and scalable solutions in robotics.</p> <p>In the simulation phase, two key components are at play: observations and depth space. These elements form the backbone of the sim-to-real pipeline for closed-loop policy deployment. Central to this process is the integration of sensor data, specifically from the RealSense 435di camera.</p> <p>The journey from raw camera input to actionable data involves several stages of processing. Initially, the raw image undergoes transformation to match the quality observed in simulation, often resulting in a compressed version devoid of multiple color channels.</p> <p>Subsequently, this modified image is subjected to further compression, ultimately yielding a latent representation of the space captured by the camera. This latent representation is then fed into a neural network model to generate a compact, 1x32 size depth latent representation of the original raw image.</p> <p>Through these intricate stages of processing and neural network compression, the project aims to bridge the gap between simulation and real-world application, ensuring seamless integration and robust performance in varied environments.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/latent-480.webp 480w,/assets/img/latent-800.webp 800w,/assets/img/latent-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/latent.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Raw terrain" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The integration of the latent representation into the trained model, alongside the observation space, serves as a critical input for generating actions, manifested as joint states. This comprehensive approach ensures that the policy developed in simulation can effectively adapt and perform in real-world scenarios.</p> <p>To facilitate this integration, a dedicated ROS2 package was developed. This package leverages the camera input and applies the compression models to generate the latent representations. By seamlessly interfacing with ROS2, the package enables efficient communication and integration with the broader robotics ecosystem.</p> <p>For a visual demonstration of the depth policy in action on the quadruped during teleoperation, please find the attached video below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/Unitree_full.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> </div> <h4 id="sim-to-real-pipeline-test"><strong>SIM TO REAL PIPELINE TEST</strong></h4> <p>A sim to real pipeline test of <a href="https://github.com/muye1202/quadruped_locomotion_project" rel="external nofollow noopener" target="_blank">quadruped_locomotion_project </a> package:</p> <div class="row"> <div class="col"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/pipeline_test.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> </div> </div> <p>This project is highly inspired from the <a href="https://github.com/chengxuxin/extreme-parkour" rel="external nofollow noopener" target="_blank">Extreme-Parkour</a> package created by CMU and is built on top of <a href="https://github.com/leggedrobotics/rsl_rl" rel="external nofollow noopener" target="_blank">RSL-RL</a> Nvidia Package.</p> <p>The code can be found at this <a href="https://github.com/sdalal1/extreme-parkour-barkour" rel="external nofollow noopener" target="_blank">github</a> link</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Shail Dalal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>